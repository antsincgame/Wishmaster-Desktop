[package]
name = "wishmaster-desktop"
version = "1.0.0"
description = "Local AI Assistant with Voice Cloning"
authors = ["Wishmaster Team"]
license = "MIT"
repository = "https://github.com/antsincgame/Wishmaster-Desktop"
edition = "2021"

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = [] }
tauri-plugin-dialog = "2"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
tokio = { version = "1", features = ["full"] }
rusqlite = { version = "0.31", features = ["bundled"] }
dirs = "5"
once_cell = "1"
base64 = "0.21"
thiserror = "1.0"  # Custom error types
shellexpand = "3"  # Shell path expansion (~, $VAR)
chrono = "0.4"     # Timestamps for logging
encoding_rs = "0.8" # UTF-8 decoder for token_to_piece (llama-cpp-2, native-llm only)

# LLM - native llama.cpp (optional; when disabled, app uses Ollama over HTTP)
llama-cpp-2 = { version = "0.1", optional = true }

# HTTP client for Ollama / remote LLM (no compile of llama.cpp)
reqwest = { version = "0.12", features = ["json", "stream"], optional = true }
futures-util = { version = "0.3", default-features = false, features = ["std"], optional = true }

# HuggingFace Hub for model downloads
hf-hub = { version = "0.4", default-features = false, features = ["ureq", "rustls-tls"] }

# Embeddings for semantic search / RAG (optional - requires glibc 2.38+)
# Using latest fastembed with native TLS for ort downloads
fastembed = { version = "5", default-features = false, features = ["ort-download-binaries", "hf-hub-native-tls"], optional = true }

# NVML for real GPU name and VRAM (compiles everywhere, runtime-only loading via libloading)
nvml-wrapper = { version = "0.11", optional = true }

# Audio - placeholder (add cpal + hound later for voice features)
# cpal = "0.15"
# hound = "3.5"

# STT - placeholder (add whisper-rs later)
# whisper-rs = "0.10"

# TTS - placeholder (add ort later for ONNX models)
# ort = "2.0"

[features]
default = ["custom-protocol", "embeddings", "ollama"]
custom-protocol = ["tauri/custom-protocol"]

# Embeddings for semantic search / RAG (requires glibc 2.38+ / Ubuntu 24.04+)
embeddings = ["dep:fastembed"]

# Ollama over HTTP (default): no LLM in binary, supports Vision models
ollama = ["dep:reqwest", "dep:futures-util"]

# Native llama.cpp in binary (optional; long build, no Vision in this build)
# Full build with native: cargo build --no-default-features --features "custom-protocol,embeddings,ollama,native-llm,cuda"
native-llm = ["dep:llama-cpp-2", "ollama"]

# CUDA for native-llm only (requires CUDA toolkit at build and runtime)
cuda = ["native-llm", "llama-cpp-2/cuda", "dep:nvml-wrapper", "nvml-wrapper"]

# NVML for GPU name/VRAM (with native-llm or cuda)
nvml-wrapper = ["dep:nvml-wrapper"]

[dev-dependencies]
tempfile = "3.10"  # For creating temp directories in tests

[profile.release]
panic = "abort"
codegen-units = 1
lto = true
opt-level = "s"
strip = false  # Required for Tauri bundler
